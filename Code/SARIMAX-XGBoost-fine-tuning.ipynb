{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 90765,
     "databundleVersionId": 10583383,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install ta",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-04T07:32:44.025819Z",
     "iopub.execute_input": "2025-01-04T07:32:44.026100Z",
     "iopub.status.idle": "2025-01-04T07:32:50.725168Z",
     "shell.execute_reply.started": "2025-01-04T07:32:44.026070Z",
     "shell.execute_reply": "2025-01-04T07:32:50.724367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting ta\n  Downloading ta-0.11.0.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ta) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ta) (2.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.16.0)\nBuilding wheels for collected packages: ta\n  Building wheel for ta (setup.py) ... \u001B[?25l\u001B[?25hdone\n  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=b714dc2952da2d301a5c27856e85c81cbe1736b34e2d0e220b5da0b5d6c7ea0f\n  Stored in directory: /root/.cache/pip/wheels/5f/67/4f/8a9f252836e053e532c6587a3230bc72a4deb16b03a829610b\nSuccessfully built ta\nInstalling collected packages: ta\nSuccessfully installed ta-0.11.0\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import f1_score\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport ta\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport xgboost as xgb\nfrom concurrent.futures import ThreadPoolExecutor\nfrom sklearn.model_selection import train_test_split\nimport gc\n\nclass TimeSeriesModel:\n    def __init__(self):\n        self.sarimax_model = None\n        self.xgb_model = None\n        self.scaler = StandardScaler()\n        self.best_params = None\n        self.best_weight = None\n        \n    def _calculate_momentum(self, df):\n        \"\"\"Calculate momentum indicators using shifted data to prevent lookahead.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        \n        # Ensure we're using shifted prices\n        close = df['close']\n        high = df['high']\n        low = df['low']\n        \n        # Calculate indicators\n        result['rsi'] = ta.momentum.RSIIndicator(close, window=14).rsi()\n        result['macd'] = ta.trend.MACD(close, window_fast=12).macd()\n        result['atr'] = ta.volatility.AverageTrueRange(high, low, close, window=14).average_true_range()\n        \n        return result\n        \n    def _calculate_volume(self, df):\n        \"\"\"Calculate volume-based indicators using shifted data.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        volume = df['volume']  # Already shifted in main function\n        \n        result['volume_ma_10'] = volume.rolling(window=10, min_periods=1).mean()\n        result['volume_ratio'] = volume / result['volume_ma_10']\n        \n        return result\n        \n    def calculate_technical_indicators(self, df):\n        \"\"\"Calculate all technical indicators ensuring no lookahead bias.\"\"\"\n        df = df.copy()\n        \n        # First shift all base price and volume data\n        base_cols = ['close', 'high', 'low', 'volume']\n        df[base_cols] = df[base_cols].shift(1)\n        \n        # Calculate returns on shifted data\n        df['returns'] = df['close'].pct_change()\n        \n        # Calculate technical indicators in parallel\n        with ThreadPoolExecutor(max_workers=2) as executor:\n            futures = {\n                executor.submit(self._calculate_momentum, df): 'momentum',\n                executor.submit(self._calculate_volume, df): 'volume'\n            }\n            for future in futures:\n                result = future.result()\n                df = pd.concat([df, result], axis=1)\n        \n        # Add time-based features\n        timestamp = pd.to_datetime(df['timestamp'], unit='s')\n        df['hour'] = timestamp.dt.hour\n        df['day_of_week'] = timestamp.dt.dayofweek\n        \n        # Calculate target (next period's price direction)\n        df['target'] = df['close'].shift(-1).diff() > 0\n        \n        return df.fillna(0)\n        \n    def prepare_features(self, df):\n        \"\"\"Prepare and scale features, ensuring no target leakage.\"\"\"\n        # Define feature columns\n        exog_columns = ['returns', 'volume_ratio', 'rsi', 'macd', 'atr', 'hour', 'day_of_week']\n        \n        # Create a copy with only the needed columns\n        feature_df = df[exog_columns].copy()\n        \n        return self.scaler.fit_transform(feature_df) if self.sarimax_model is None else self.scaler.transform(feature_df)\n        \n    def tune_parameters(self, train_data):\n        \"\"\"Tune model parameters using a proper temporal train-validation split.\"\"\"\n        best_params = None\n        best_weight = None\n        best_score = 0\n        print(\"Tuning parameters...\")\n        df = self.calculate_technical_indicators(train_data)\n        \n        # Ensure chronological split\n        train_size = int(len(df) * 0.7)\n        train_end = train_size\n        train_start = max(0, train_end - 500000)  # Use last 500K rows of training set\n        \n        train_df = df[train_start:train_end]\n        val_df = df[train_end:-1]  # -1 to exclude last row where target is NA\n        \n        if len(train_df) == 0:\n            train_df = df[-500000:-100000]\n            val_df = df[-100000:-1]\n            \n        print(f\"Train set size: {len(train_df)}, Validation set size: {len(val_df)}\")\n        print(f\"Validation set positive ratio: {val_df['target'].mean()}\")\n        \n        xgb_params = {\n            'learning_rate': [0.03, 0.05],\n            'max_depth': [5, 6],\n            'n_estimators': [200],\n            'subsample': [0.9]\n        }\n        \n        sarimax_orders = [(1,1,0), (2,1,0)]\n        seasonal_orders = [(0,1,1,12)]\n        weights = [0.2, 0.3, 0.4]\n        \n        total_combinations = (\n            len(sarimax_orders) * len(seasonal_orders) * \n            len(xgb_params['learning_rate']) * len(xgb_params['max_depth']) * \n            len(xgb_params['n_estimators']) * len(xgb_params['subsample']) * len(weights)\n        )\n        print(f\"\\nStarting grid search with {total_combinations} combinations\")\n        \n        best_score = 0\n        total_tried = 0\n        \n        for order in sarimax_orders:\n            for seasonal_order in seasonal_orders:\n                print(f\"\\nSARIMAX order {order}, seasonal_order {seasonal_order}\")\n                for lr in xgb_params['learning_rate']:\n                    for depth in xgb_params['max_depth']:\n                        for n_est in xgb_params['n_estimators']:\n                            for subsample in xgb_params['subsample']:\n                                for weight in weights:\n                                    try:\n                                        total_tried += 1\n                                        print(f\"\\nTrial {total_tried}/{total_combinations}\")\n                                        print(f\"XGB: lr={lr}, depth={depth}, n_est={n_est}, subsample={subsample}, weight={weight}\")\n                                        \n                                        # Train SARIMAX\n                                        print(\"Training SARIMAX...\")\n                                        # Use last 50K points of training data for SARIMAX\n                                        sarimax_train = train_df.iloc[-50000:]\n                                        train_exog = self.prepare_features(sarimax_train)\n                                        val_exog = self.prepare_features(val_df)\n                                        \n                                        sarimax = SARIMAX(\n                                            sarimax_train['close'],\n                                            exog=train_exog,\n                                            order=order,\n                                            seasonal_order=seasonal_order\n                                        ).fit(disp=False, method='powell')\n                                        \n                                        # Train XGBoost\n                                        print(\"Training XGBoost...\")\n                                        xgb_model = xgb.XGBClassifier(\n                                            learning_rate=lr,\n                                            max_depth=depth,\n                                            n_estimators=n_est,\n                                            subsample=subsample,\n                                            tree_method='hist',\n                                            device='cuda'\n                                        )\n                                        \n                                        X_train = self.prepare_features(train_df)\n                                        y_train = train_df['target'].astype(int)\n                                        xgb_model.fit(X_train, y_train)\n                                        \n                                        # Make predictions\n                                        sarimax_pred = sarimax.forecast(steps=len(val_df), exog=val_exog)\n                                        xgb_pred = xgb_model.predict_proba(val_exog)[:, 1]\n                                        \n                                        final_pred = (\n                                            (sarimax_pred.diff() > 0).astype(float) * weight + \n                                            xgb_pred * (1-weight) > 0.5\n                                        ).astype(int)\n                                        \n                                        # Calculate F1 score\n                                        y_val = val_df['target'].astype(int)\n                                        score = f1_score(y_val, final_pred, average='macro')\n                                        print(f\"F1 Score: {score}\")\n                                        \n                                        if score > best_score:\n                                            best_score = score\n                                            best_params = {\n                                                'order': order,\n                                                'seasonal_order': seasonal_order,\n                                                'learning_rate': lr,\n                                                'max_depth': depth,\n                                                'n_estimators': n_est,\n                                                'subsample': subsample\n                                            }\n                                            best_weight = weight\n                                            print(f\"New best F1: {best_score}\")\n                                            print(f\"Best params: {best_params}\")\n                                            print(f\"Best weight: {best_weight}\")\n                                            \n                                    except Exception as e:\n                                        print(f\"Error in trial: {str(e)}\")\n                                        continue\n        \n        if best_params is None:\n            raise Exception(\"No valid parameters found during tuning\")\n            \n        self.best_params = best_params\n        self.best_weight = best_weight\n        print(f\"\\nTuning completed. Best F1: {best_score}\")\n        return best_params, best_weight\n        \n    def train(self, train_data):\n        \"\"\"Train the final model using the entire training dataset.\"\"\"\n        df = self.calculate_technical_indicators(train_data)\n        \n        if self.best_params is None:\n            self.best_params, self.best_weight = self.tune_parameters(train_data)\n        \n        print(\"\\nTraining final models...\")\n        # Use last 50K points for SARIMAX\n        sarimax_df = df.iloc[-50000:-1].copy()  # Exclude last row where target is NA\n        sarimax_exog = self.prepare_features(sarimax_df)\n        \n        self.sarimax_model = SARIMAX(\n            sarimax_df['close'],\n            exog=sarimax_exog,\n            order=self.best_params['order'],\n            seasonal_order=self.best_params['seasonal_order']\n        ).fit(disp=False, method='powell')\n        \n        # Train XGBoost on all data\n        X = self.prepare_features(df[:-1])  # Exclude last row where target is NA\n        y = df[:-1]['target'].astype(int)\n        \n        self.xgb_model = xgb.XGBClassifier(\n            learning_rate=self.best_params['learning_rate'],\n            max_depth=self.best_params['max_depth'],\n            n_estimators=self.best_params['n_estimators'],\n            subsample=self.best_params['subsample'],\n            tree_method='hist',\n            device='cuda'\n        )\n        self.xgb_model.fit(X, y)\n        gc.collect()\n        \n    def predict(self, future_df):\n        \"\"\"Make predictions for future data.\"\"\"\n        print(\"Making predictions...\")\n        df = self.calculate_technical_indicators(future_df)\n        exog = self.prepare_features(df)\n        \n        sarimax_forecast = self.sarimax_model.forecast(steps=len(df), exog=exog)\n        xgb_pred = self.xgb_model.predict_proba(exog)[:, 1]\n        \n        return (\n            (sarimax_forecast.diff() > 0).astype(float) * self.best_weight + \n            xgb_pred * (1-self.best_weight) > 0.5\n        ).astype(int)\n\ndef main():\n    try:\n        print(\"Loading data...\")\n        chunks = []\n        for chunk in pd.read_csv('/kaggle/input/directional-forecasting-cryptocurrencies/train.csv',\n                               usecols=['timestamp', 'close', 'high', 'low', 'volume'],\n                               chunksize=500000):\n            chunks.append(chunk)\n        \n        train_df = pd.concat(chunks)\n        chunks = None\n        gc.collect()\n        \n        test_df = pd.read_csv('/kaggle/input/directional-forecasting-cryptocurrencies/test.csv',\n                             usecols=['timestamp', 'close', 'high', 'low', 'volume'])\n        \n        model = TimeSeriesModel()\n        try:\n            model.train(train_df)\n        except Exception as e:\n            print(f\"Error during model training: {str(e)}\")\n            raise\n            \n        try:\n            predictions = model.predict(test_df)\n            submission = pd.DataFrame({\n                'row_id': range(len(predictions)), \n                'target': predictions\n            })\n            submission.to_csv('submission.csv', index=False)\n            print(\"Done!\")\n        except Exception as e:\n            print(f\"Error during prediction or saving: {str(e)}\")\n            raise\n            \n    except Exception as e:\n        print(f\"Fatal error in main: {str(e)}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-04T04:36:46.450160Z",
     "iopub.execute_input": "2025-01-04T04:36:46.450555Z",
     "iopub.status.idle": "2025-01-04T06:45:28.695373Z",
     "shell.execute_reply.started": "2025-01-04T04:36:46.450520Z",
     "shell.execute_reply": "2025-01-04T06:45:28.694481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading data...\nTuning parameters...\nTrain set size: 500000, Validation set size: 636731\nValidation set positive ratio: 0.47318569380162107\n\nStarting grid search with 24 combinations\n\nSARIMAX order (1, 1, 0), seasonal_order (0, 1, 1, 12)\n\nTrial 1/24\nXGB: lr=0.03, depth=5, n_est=200, subsample=0.9, weight=0.2\nTraining SARIMAX...\nTraining XGBoost...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [04:40:48] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "F1 Score: 0.49338035243545053\nNew best F1: 0.49338035243545053\nBest params: {'order': (1, 1, 0), 'seasonal_order': (0, 1, 1, 12), 'learning_rate': 0.03, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.9}\nBest weight: 0.2\n\nTrial 2/24\nXGB: lr=0.03, depth=5, n_est=200, subsample=0.9, weight=0.3\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.49199780373933566\n\nTrial 3/24\nXGB: lr=0.03, depth=5, n_est=200, subsample=0.9, weight=0.4\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4919846242185263\n\nTrial 4/24\nXGB: lr=0.03, depth=6, n_est=200, subsample=0.9, weight=0.2\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4938281043367362\nNew best F1: 0.4938281043367362\nBest params: {'order': (1, 1, 0), 'seasonal_order': (0, 1, 1, 12), 'learning_rate': 0.03, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.9}\nBest weight: 0.2\n\nTrial 5/24\nXGB: lr=0.03, depth=6, n_est=200, subsample=0.9, weight=0.3\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.49201074992211624\n\nTrial 6/24\nXGB: lr=0.03, depth=6, n_est=200, subsample=0.9, weight=0.4\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4919862341588254\n\nTrial 7/24\nXGB: lr=0.05, depth=5, n_est=200, subsample=0.9, weight=0.2\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4949130330246432\nNew best F1: 0.4949130330246432\nBest params: {'order': (1, 1, 0), 'seasonal_order': (0, 1, 1, 12), 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.9}\nBest weight: 0.2\n\nTrial 8/24\nXGB: lr=0.05, depth=5, n_est=200, subsample=0.9, weight=0.3\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.49201502281304377\n\nTrial 9/24\nXGB: lr=0.05, depth=5, n_est=200, subsample=0.9, weight=0.4\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.491987844096373\n\nTrial 10/24\nXGB: lr=0.05, depth=6, n_est=200, subsample=0.9, weight=0.2\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4950922822915762\nNew best F1: 0.4950922822915762\nBest params: {'order': (1, 1, 0), 'seasonal_order': (0, 1, 1, 12), 'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.9}\nBest weight: 0.2\n\nTrial 11/24\nXGB: lr=0.05, depth=6, n_est=200, subsample=0.9, weight=0.3\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.49208095039278865\n\nTrial 12/24\nXGB: lr=0.05, depth=6, n_est=200, subsample=0.9, weight=0.4\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.491987844096373\n\nSARIMAX order (2, 1, 0), seasonal_order (0, 1, 1, 12)\n\nTrial 13/24\nXGB: lr=0.03, depth=5, n_est=200, subsample=0.9, weight=0.2\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4936789287441612\n\nTrial 14/24\nXGB: lr=0.03, depth=5, n_est=200, subsample=0.9, weight=0.3\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.492310482212583\n\nTrial 15/24\nXGB: lr=0.03, depth=5, n_est=200, subsample=0.9, weight=0.4\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.49230043070428586\n\nTrial 16/24\nXGB: lr=0.03, depth=6, n_est=200, subsample=0.9, weight=0.2\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.49408826529252176\n\nTrial 17/24\nXGB: lr=0.03, depth=6, n_est=200, subsample=0.9, weight=0.3\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4923249575302272\n\nTrial 18/24\nXGB: lr=0.03, depth=6, n_est=200, subsample=0.9, weight=0.4\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4923020435296893\n\nTrial 19/24\nXGB: lr=0.05, depth=5, n_est=200, subsample=0.9, weight=0.2\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4951223925234936\nNew best F1: 0.4951223925234936\nBest params: {'order': (2, 1, 0), 'seasonal_order': (0, 1, 1, 12), 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.9}\nBest weight: 0.2\n\nTrial 20/24\nXGB: lr=0.05, depth=5, n_est=200, subsample=0.9, weight=0.3\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4923369878716205\n\nTrial 21/24\nXGB: lr=0.05, depth=5, n_est=200, subsample=0.9, weight=0.4\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4923036563523244\n\nTrial 22/24\nXGB: lr=0.05, depth=6, n_est=200, subsample=0.9, weight=0.2\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4953731240344456\nNew best F1: 0.4953731240344456\nBest params: {'order': (2, 1, 0), 'seasonal_order': (0, 1, 1, 12), 'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.9}\nBest weight: 0.2\n\nTrial 23/24\nXGB: lr=0.05, depth=6, n_est=200, subsample=0.9, weight=0.3\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4923947686981311\n\nTrial 24/24\nXGB: lr=0.05, depth=6, n_est=200, subsample=0.9, weight=0.4\nTraining SARIMAX...\nTraining XGBoost...\nF1 Score: 0.4923036563523244\n\nTuning completed. Best F1: 0.4953731240344456\n\nTraining final models...\nMaking predictions...\nDone!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
