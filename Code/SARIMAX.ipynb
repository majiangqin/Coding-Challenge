{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f994f2d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T04:40:14.532777Z",
     "iopub.status.busy": "2024-12-29T04:40:14.532391Z",
     "iopub.status.idle": "2024-12-29T04:40:20.941092Z",
     "shell.execute_reply": "2024-12-29T04:40:20.939909Z"
    },
    "papermill": {
     "duration": 6.414703,
     "end_time": "2024-12-29T04:40:20.942889",
     "exception": false,
     "start_time": "2024-12-29T04:40:14.528186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ta\r\n",
      "  Downloading ta-0.11.0.tar.gz (25 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ta) (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ta) (2.1.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.16.0)\r\n",
      "Building wheels for collected packages: ta\r\n",
      "  Building wheel for ta (setup.py) ... \u001B[?25l\u001B[?25hdone\r\n",
      "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=39df4c0dd33ac3f32ffa082aca106a9f74d412d8087a191926665d5e8cb01413\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/67/4f/8a9f252836e053e532c6587a3230bc72a4deb16b03a829610b\r\n",
      "Successfully built ta\r\n",
      "Installing collected packages: ta\r\n",
      "Successfully installed ta-0.11.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a374b4db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T04:40:20.949483Z",
     "iopub.status.busy": "2024-12-29T04:40:20.949235Z",
     "iopub.status.idle": "2024-12-29T04:40:22.505170Z",
     "shell.execute_reply": "2024-12-29T04:40:22.504434Z"
    },
    "papermill": {
     "duration": 1.560971,
     "end_time": "2024-12-29T04:40:22.506960",
     "exception": false,
     "start_time": "2024-12-29T04:40:20.945989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "import ta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba947604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T04:40:22.513164Z",
     "iopub.status.busy": "2024-12-29T04:40:22.512805Z",
     "iopub.status.idle": "2024-12-29T04:40:26.790534Z",
     "shell.execute_reply": "2024-12-29T04:40:26.789475Z"
    },
    "papermill": {
     "duration": 4.282375,
     "end_time": "2024-12-29T04:40:26.792077",
     "exception": false,
     "start_time": "2024-12-29T04:40:22.509702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data date range:\n",
      "Start: 2018-05-04 22:01:00\n",
      "End: 2022-05-17 19:58:00\n",
      "Total days: 1473\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/directional-forecasting-cryptocurrencies/train.csv')\n",
    "\n",
    "train_df['datetime'] = pd.to_datetime(train_df['timestamp'], unit='s')\n",
    "print(f\"Training data date range:\")\n",
    "print(f\"Start: {train_df['datetime'].min()}\")\n",
    "print(f\"End: {train_df['datetime'].max()}\")\n",
    "print(f\"Total days: {(train_df['datetime'].max() - train_df['datetime'].min()).days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad3874c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T04:40:26.798473Z",
     "iopub.status.busy": "2024-12-29T04:40:26.798235Z",
     "iopub.status.idle": "2024-12-29T04:40:26.803478Z",
     "shell.execute_reply": "2024-12-29T04:40:26.802813Z"
    },
    "papermill": {
     "duration": 0.009819,
     "end_time": "2024-12-29T04:40:26.804794",
     "exception": false,
     "start_time": "2024-12-29T04:40:26.794975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2122438, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2300313f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T04:40:26.811083Z",
     "iopub.status.busy": "2024-12-29T04:40:26.810678Z",
     "iopub.status.idle": "2024-12-29T04:40:28.485854Z",
     "shell.execute_reply": "2024-12-29T04:40:28.484801Z"
    },
    "papermill": {
     "duration": 1.679895,
     "end_time": "2024-12-29T04:40:28.487509",
     "exception": false,
     "start_time": "2024-12-29T04:40:26.807614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data date range:\n",
      "Start: 2022-05-17 19:58:00\n",
      "End: 2024-02-08 12:14:00\n",
      "Total days: 631\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('/kaggle/input/directional-forecasting-cryptocurrencies/test.csv')\n",
    "test_df['datetime'] = pd.to_datetime(test_df['timestamp'],unit='s')\n",
    "print(f\"Test data date range:\")\n",
    "print(f\"Start: {test_df['datetime'].min()}\")\n",
    "print(f\"End: {test_df['datetime'].max()}\")\n",
    "print(f\"Total days: {(test_df['datetime'].max() - test_df['datetime'].min()).days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9f1dd83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T04:40:28.494377Z",
     "iopub.status.busy": "2024-12-29T04:40:28.494107Z",
     "iopub.status.idle": "2024-12-29T04:40:28.499008Z",
     "shell.execute_reply": "2024-12-29T04:40:28.498287Z"
    },
    "papermill": {
     "duration": 0.009759,
     "end_time": "2024-12-29T04:40:28.500227",
     "exception": false,
     "start_time": "2024-12-29T04:40:28.490468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(909617, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d208dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T04:40:28.506461Z",
     "iopub.status.busy": "2024-12-29T04:40:28.506208Z",
     "iopub.status.idle": "2024-12-29T05:35:50.612643Z",
     "shell.execute_reply": "2024-12-29T05:35:50.611296Z"
    },
    "papermill": {
     "duration": 3322.111145,
     "end_time": "2024-12-29T05:35:50.614182",
     "exception": false,
     "start_time": "2024-12-29T04:40:28.503037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (2122438, 11)\n",
      "Test data shape: (909617, 11)\n",
      "\n",
      "Predicting on the entire test set...\n",
      "\n",
      "Submission file saved to submission.csv\n",
      "Total predictions made: 909617\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import ta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def calculate_macro_f1(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Optional F1 utility.\"\"\"\n",
    "    f1_class_0 = f1_score(y_true, y_pred, pos_label=0, average='binary')\n",
    "    f1_class_1 = f1_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "    return (f1_class_0 + f1_class_1) / 2\n",
    "\n",
    "class TimeSeriesModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculates technical indicators. \n",
    "        Expects columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume'] at a minimum.\n",
    "        \"\"\"\n",
    "        required_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        # Calculate indicators (shifted columns, RSI, MACD, etc.)\n",
    "        df['returns'] = df['close'].shift(1)\n",
    "        df['rsi'] = ta.momentum.RSIIndicator(df['close']).rsi()\n",
    "        df['macd'] = ta.trend.MACD(df['close']).macd()\n",
    "\n",
    "        df['rolling_mean_5'] = df['close'].rolling(window=5, min_periods=1).mean()\n",
    "        df['volume_ma_5'] = df['volume'].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "        df['price_range'] = (df['high'] - df['low']) / df['open']\n",
    "\n",
    "        # Time features\n",
    "        df['hour'] = pd.to_datetime(df['timestamp'], unit='s').dt.hour\n",
    "        df['day_of_week'] = pd.to_datetime(df['timestamp'], unit='s').dt.dayofweek\n",
    "\n",
    "        # Shift columns (except timestamp, target, hour, day_of_week)\n",
    "        # to avoid label leakage\n",
    "        excluded_cols = ['timestamp', 'target', 'hour', 'day_of_week']\n",
    "        feature_cols = [c for c in df.columns if c not in excluded_cols]\n",
    "        df[feature_cols] = df[feature_cols].shift(1)\n",
    "\n",
    "        # Sort by timestamp just to be safe\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def prepare_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Selects and scales exogenous features.\n",
    "        \"\"\"\n",
    "        exog_columns = [\n",
    "            'returns', 'rsi', 'macd',\n",
    "            'rolling_mean_5', 'volume_ma_5', 'price_range',\n",
    "            'hour', 'day_of_week'\n",
    "        ]\n",
    "        for col in exog_columns:\n",
    "            if col not in df.columns:\n",
    "                # If a column is missing, create it filled with 0\n",
    "                df[col] = 0\n",
    "\n",
    "        # Fill any leftover NaNs with 0; we've already done fillna but just in case.\n",
    "        exog = df[exog_columns].fillna(0)\n",
    "\n",
    "        # Fit scaler if model is None, otherwise transform\n",
    "        if self.model is None:\n",
    "            return self.scaler.fit_transform(exog)\n",
    "        else:\n",
    "            return self.scaler.transform(exog)\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Train SARIMAX on the entire training set (no chunking).\n",
    "        \"\"\"\n",
    "        if len(train_data) == 0:\n",
    "            raise ValueError(\"train_data is empty, cannot train.\")\n",
    "\n",
    "        # 1) Calculate indicators\n",
    "        df = self.calculate_technical_indicators(train_data)\n",
    "\n",
    "        # 2) Fill NaNs instead of drop\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "        # Optionally, you can replace any remaining NaNs with 0:\n",
    "        # df = df.fillna(0)\n",
    "\n",
    "        # 3) Check if DataFrame is still empty\n",
    "        if df.empty:\n",
    "            raise ValueError(\"No valid rows after fillna. Possibly all data was NaN.\")\n",
    "\n",
    "        # 4) Prepare exogenous features\n",
    "        exog = self.prepare_features(df)\n",
    "\n",
    "        # 5) Train the SARIMAX model\n",
    "        self.model = SARIMAX(\n",
    "            df['close'],\n",
    "            exog=exog,\n",
    "            order=(1, 0, 0),          # Simple AR(1) example\n",
    "            enforce_stationarity=False\n",
    "        ).fit(disp=False)\n",
    "\n",
    "    def predict(self, future_df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict on the entire test set using the trained model.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model needs to be trained first.\")\n",
    "\n",
    "        if len(future_df) == 0:\n",
    "            print(\"Warning: future_df is empty. Returning empty predictions.\")\n",
    "            return np.array([])\n",
    "\n",
    "        # 1) Calculate indicators\n",
    "        df = self.calculate_technical_indicators(future_df)\n",
    "\n",
    "        # 2) Fill NaNs instead of drop\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "        # Or, df = df.fillna(0)\n",
    "\n",
    "        if df.empty:\n",
    "            print(\"Warning: No valid rows after fillna in predict. Returning empty.\")\n",
    "            return np.array([])\n",
    "\n",
    "        # 3) Prepare exogenous features\n",
    "        exog = self.prepare_features(df)\n",
    "\n",
    "        # 4) Forecast\n",
    "        forecast = self.model.forecast(steps=len(df), exog=exog)\n",
    "\n",
    "        # 5) Compute directional predictions\n",
    "        diff_forecast = forecast.diff()\n",
    "        predictions = (diff_forecast > 0).astype(int)\n",
    "\n",
    "        # Edge case for the first row\n",
    "        if len(predictions) > 0:\n",
    "            # Compare forecast[0] to df['close'].iloc[0]\n",
    "            predictions.iloc[0] = int(forecast.iloc[0] > df['close'].iloc[0])\n",
    "\n",
    "        return predictions.values\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # 1) Load train & test data\n",
    "        train_df = pd.read_csv('/kaggle/input/directional-forecasting-cryptocurrencies/train.csv')\n",
    "        test_df = pd.read_csv('/kaggle/input/directional-forecasting-cryptocurrencies/test.csv')\n",
    "\n",
    "        print(\"Training data shape:\", train_df.shape)\n",
    "        print(\"Test data shape:\", test_df.shape)\n",
    "\n",
    "        # 2) Train on the entire training dataset\n",
    "        model = TimeSeriesModel()\n",
    "        model.train(train_df)\n",
    "\n",
    "        # 3) Predict on the entire test dataset\n",
    "        print(\"\\nPredicting on the entire test set...\")\n",
    "        predictions = model.predict(test_df)\n",
    "\n",
    "        # 4) Check if length matches\n",
    "        if len(predictions) != len(test_df):\n",
    "            print(f\"Warning: predictions length ({len(predictions)}) \"\n",
    "                  f\"does not match test_df length ({len(test_df)})\")\n",
    "\n",
    "        # 5) Save to submission.csv\n",
    "        output_path = 'submission.csv'\n",
    "        pd.DataFrame({\n",
    "            'row_id': range(len(predictions)),\n",
    "            'target': predictions\n",
    "        }).to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"\\nSubmission file saved to {output_path}\")\n",
    "        print(f\"Total predictions made: {len(predictions)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a6b4e",
   "metadata": {
    "papermill": {
     "duration": 0.002636,
     "end_time": "2024-12-29T05:35:50.620182",
     "exception": false,
     "start_time": "2024-12-29T05:35:50.617546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "0.49092"
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10583383,
     "sourceId": 90765,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3338.569892,
   "end_time": "2024-12-29T05:35:51.244544",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-29T04:40:12.674652",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
