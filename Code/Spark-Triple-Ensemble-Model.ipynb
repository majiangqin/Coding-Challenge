{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 90765,
     "databundleVersionId": 10583383,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install ta",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-05T19:22:58.608156Z",
     "iopub.execute_input": "2025-01-05T19:22:58.608714Z",
     "iopub.status.idle": "2025-01-05T19:23:02.723221Z",
     "shell.execute_reply.started": "2025-01-05T19:22:58.608667Z",
     "shell.execute_reply": "2025-01-05T19:23:02.721960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: ta in /usr/local/lib/python3.10/dist-packages (0.11.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ta) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ta) (2.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.16.0)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "!pip install pyspark",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-05T19:23:02.725092Z",
     "iopub.execute_input": "2025-01-05T19:23:02.725510Z",
     "iopub.status.idle": "2025-01-05T19:23:06.856898Z",
     "shell.execute_reply.started": "2025-01-05T19:23:02.725473Z",
     "shell.execute_reply": "2025-01-05T19:23:06.855702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.4)\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SparkTripleEnsembleModel:\n",
    "    \"\"\"\n",
    "    A class to train and predict using an ensemble of GBT, RF, and LSTM models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spark=None, num_partitions=200):\n",
    "        print(\"Initializing Spark session...\")\n",
    "        self.spark = (\n",
    "            spark if spark else\n",
    "            SparkSession.builder\n",
    "                .appName(\"TripleEnsembleModel\")\n",
    "                .config(\"spark.driver.memory\", \"16g\")\n",
    "                .config(\"spark.executor.memory\", \"16g\")\n",
    "                .config(\"spark.memory.offHeap.enabled\", True)\n",
    "                .config(\"spark.memory.offHeap.size\", \"16g\")\n",
    "                .config(\"spark.driver.maxResultSize\", \"8g\")\n",
    "                .config(\"spark.python.worker.memory\", \"8g\")\n",
    "                .config(\"spark.sql.shuffle.partitions\", str(num_partitions))\n",
    "                .config(\"spark.default.parallelism\", str(num_partitions))\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "                .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "                .getOrCreate()\n",
    "        )\n",
    "        print(\"Spark session initialized successfully.\")\n",
    "        self.num_partitions = num_partitions\n",
    "                    # Update feature columns list\n",
    "        self.feature_cols = [\n",
    "        'returns', 'volume_ratio', 'rsi', 'macd',\n",
    "        'bb_upper', 'bb_lower', 'bb_pct'  # New features\n",
    "        ]\n",
    "        self.gbt_pipeline = None\n",
    "        self.rf_pipeline = None\n",
    "        self.lstm_model = None\n",
    "        print(\"Model instance initialized.\")\n",
    "\n",
    "    def _calculate_features(self, df):\n",
    "        print(\"Starting feature calculation...\")\n",
    "        df = df.repartition(self.num_partitions)\n",
    "        print(\"Repartitioned data for parallel processing.\")\n",
    "\n",
    "        w = Window.orderBy('timestamp')\n",
    "        w_10 = Window.orderBy('timestamp').rowsBetween(-10, -1)\n",
    "        w_14 = Window.orderBy('timestamp').rowsBetween(-14, -1)\n",
    "        w_20 = Window.orderBy('timestamp').rowsBetween(-20, -1) \n",
    "\n",
    "        print(\"Calculating returns and volume ratio...\")\n",
    "        df = (\n",
    "            df.withColumn('prev_close', F.lag('close').over(w))\n",
    "              .withColumn('returns', (F.col('close') - F.col('prev_close')) / F.col('prev_close'))\n",
    "              .withColumn('volume_ma', F.avg('volume').over(w_10))\n",
    "              .withColumn('volume_ratio', F.col('volume') / F.col('volume_ma'))\n",
    "              .drop('prev_close', 'volume_ma')\n",
    "        )\n",
    "\n",
    "        print(\"Calculating RSI...\")\n",
    "        df = (\n",
    "            df.withColumn('price_diff', F.col('close') - F.lag('close', 1).over(w))\n",
    "              .withColumn('gain', F.when(F.col('price_diff') > 0, F.col('price_diff')).otherwise(0))\n",
    "              .withColumn('loss', F.when(F.col('price_diff') < 0, -F.col('price_diff')).otherwise(0))\n",
    "              .withColumn('avg_gain', F.avg('gain').over(w_14))\n",
    "              .withColumn('avg_loss', F.avg('loss').over(w_14))\n",
    "              .withColumn('rs', F.col('avg_gain') / F.col('avg_loss'))\n",
    "              .withColumn('rsi', 100 - (100 / (1 + F.col('rs'))))\n",
    "              .drop('price_diff', 'gain', 'loss', 'avg_gain', 'avg_loss', 'rs')\n",
    "        )\n",
    "\n",
    "        print(\"Calculating MACD...\")\n",
    "        df = (\n",
    "            df.withColumn('ema12', F.avg('close').over(Window.orderBy('timestamp').rowsBetween(-12, -1)))\n",
    "              .withColumn('ema26', F.avg('close').over(Window.orderBy('timestamp').rowsBetween(-26, -1)))\n",
    "              .withColumn('macd', F.col('ema12') - F.col('ema26'))\n",
    "              .drop('ema12', 'ema26')\n",
    "        )\n",
    "\n",
    "            # Add Bollinger Bands\n",
    "        df = (\n",
    "        df.withColumn('sma20', F.avg('close').over(w_20))\n",
    "          .withColumn('stddev20', F.stddev('close').over(w_20))\n",
    "          .withColumn('bb_upper', F.col('sma20') + F.col('stddev20') * 2)\n",
    "          .withColumn('bb_lower', F.col('sma20') - F.col('stddev20') * 2)\n",
    "          .withColumn('bb_pct', (F.col('close') - F.col('bb_lower')) / (F.col('bb_upper') - F.col('bb_lower')))\n",
    "          .drop('sma20', 'stddev20') \n",
    "        )\n",
    "\n",
    "        if 'target' not in df.columns:\n",
    "            print(\"Calculating target column...\")\n",
    "            df = (\n",
    "                df.withColumn('next_close', F.lead('close', 1).over(w))\n",
    "                  .withColumn('target', F.when(F.col('next_close') > F.col('close'), 1).otherwise(0))\n",
    "                  .drop('next_close')\n",
    "            )\n",
    "\n",
    "\n",
    "        keep_cols = ['timestamp'] + self.feature_cols + (['target'] if 'target' in df.columns else [])\n",
    "        df = df.select(keep_cols).na.fill(0).cache()\n",
    "        print(f\"Feature calculation complete. Number of rows: {df.count()}\")\n",
    "        return df\n",
    "\n",
    "    def _prepare_lstm_data(self, pdf, sequence_length=10):\n",
    "        print(f\"Preparing LSTM data with sequence length {sequence_length}...\")\n",
    "        data = pdf[self.feature_cols].values\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - sequence_length):\n",
    "            X.append(data[i:(i + sequence_length)])\n",
    "            if 'target' in pdf.columns:\n",
    "                y.append(pdf['target'].iloc[i + sequence_length])\n",
    "        print(f\"LSTM data preparation complete. Samples: {len(X)}\")\n",
    "        return np.array(X), np.array(y) if y else None\n",
    "\n",
    "    def _build_lstm_model(self, input_shape):\n",
    "        print(\"Building LSTM model...\")\n",
    "        model = Sequential([\n",
    "            LSTM(32, input_shape=input_shape, return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(16),\n",
    "            Dropout(0.1),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        print(\"LSTM model built and compiled.\")\n",
    "        return model\n",
    "\n",
    "    def train(self, train_data):\n",
    "        \"\"\"\n",
    "        Train ensemble models, using most recent data for LSTM.\n",
    "        \"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        df = self._calculate_features(train_data).cache()\n",
    "        total_rows = df.count()\n",
    "        print(f\"Total training rows: {total_rows}\")\n",
    "\n",
    "        print(\"Training GBT model...\")\n",
    "        assembler = VectorAssembler(inputCols=self.feature_cols, outputCol=\"features\")\n",
    "        scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "        gbt = GBTClassifier(featuresCol=\"scaled_features\", labelCol=\"target\", maxDepth=6, maxBins=32, maxIter=100)\n",
    "        self.gbt_pipeline = Pipeline(stages=[assembler, scaler, gbt]).fit(df)\n",
    "        print(\"GBT training completed.\")\n",
    "\n",
    "        print(\"Training RF model...\")\n",
    "        rf = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"target\", numTrees=100, maxDepth=6, maxBins=32)\n",
    "        self.rf_pipeline = Pipeline(stages=[assembler, scaler, rf]).fit(df)\n",
    "        print(\"RF training completed.\")\n",
    "\n",
    "        print(\"Training LSTM model...\")\n",
    "        recent_data = df.orderBy(F.col('timestamp').desc())\n",
    "        recent_data = recent_data.orderBy('timestamp')\n",
    "        train_pdf = recent_data.select(['timestamp'] + self.feature_cols + ['target']).toPandas()\n",
    "\n",
    "        X_lstm, y_lstm = self._prepare_lstm_data(train_pdf)\n",
    "        self.lstm_model = self._build_lstm_model((X_lstm.shape[1], X_lstm.shape[2]))\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "        self.lstm_model.fit(X_lstm, y_lstm, epochs=20, batch_size=512, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        \"\"\"\n",
    "        Predict using the ensemble with exact length matching.\n",
    "        \"\"\"\n",
    "        print(\"Starting prediction...\")\n",
    "        df = self._calculate_features(test_data).cache()\n",
    "        gbt_probs = self.gbt_pipeline.transform(df).select(\n",
    "            F.udf(lambda v: float(v[1]), returnType=DoubleType())('probability').alias('probability')\n",
    "        ).toPandas()['probability'].values\n",
    "\n",
    "        rf_probs = self.rf_pipeline.transform(df).select(\n",
    "            F.udf(lambda v: float(v[1]), returnType=DoubleType())('probability').alias('probability')\n",
    "        ).toPandas()['probability'].values\n",
    "\n",
    "        test_pdf = df.select(['timestamp'] + self.feature_cols).toPandas()\n",
    "        X_lstm, _ = self._prepare_lstm_data(test_pdf)\n",
    "        lstm_pred = self.lstm_model.predict(X_lstm).flatten()\n",
    "\n",
    "        padded_lstm = np.zeros(df.count())\n",
    "        padded_lstm[10:] = lstm_pred\n",
    "\n",
    "        ensemble_pred = gbt_probs * 0.5 + rf_probs * 0.5\n",
    "        final_predictions = (ensemble_pred > 0.5).astype(int)\n",
    "        return final_predictions\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(\"CryptoForecasting\").getOrCreate()\n",
    "        train_df = spark.read.csv('/kaggle/input/directional-forecasting-cryptocurrencies/train.csv', header=True, inferSchema=True).select('timestamp', 'close', 'high', 'low', 'volume')\n",
    "        test_df = spark.read.csv('/kaggle/input/directional-forecasting-cryptocurrencies/test.csv', header=True, inferSchema=True).select('timestamp', 'close', 'high', 'low', 'volume')\n",
    "\n",
    "        model = SparkTripleEnsembleModel(spark)\n",
    "        model.train(train_df)\n",
    "        predictions = model.predict(test_df)\n",
    "        pd.DataFrame({'row_id': range(len(predictions)), 'target': predictions}).to_csv('submission.csv', index=False)\n",
    "\n",
    "        spark.stop()\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-05T20:07:12.557757Z",
     "iopub.execute_input": "2025-01-05T20:07:12.558534Z",
     "iopub.status.idle": "2025-01-05T20:37:27.828434Z",
     "shell.execute_reply.started": "2025-01-05T20:07:12.558493Z",
     "shell.execute_reply": "2025-01-05T20:37:27.827367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Initializing Spark session...\nSpark session initialized successfully.\nModel instance initialized.\nStarting training...\nStarting feature calculation...\nRepartitioned data for parallel processing.\nCalculating returns and volume ratio...\nCalculating RSI...\nCalculating MACD...\nCalculating target column...\nFeature calculation complete. Number of rows: 2122438\nTotal training rows: 2122438\nTraining GBT model...\nGBT training completed.\nTraining RF model...\nRF training completed.\nTraining LSTM model...\nPreparing LSTM data with sequence length 10...\nLSTM data preparation complete. Samples: 2122428\nBuilding LSTM model...\nLSTM model built and compiled.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 1/20\n\u001B[1m3731/3731\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m89s\u001B[0m 23ms/step - accuracy: 0.5215 - loss: 0.6924 - val_accuracy: 0.5392 - val_loss: 0.6906\nEpoch 2/20\n\u001B[1m3731/3731\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m85s\u001B[0m 23ms/step - accuracy: 0.5221 - loss: 0.6921 - val_accuracy: 0.5392 - val_loss: 0.6902\nEpoch 3/20\n\u001B[1m3731/3731\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m85s\u001B[0m 23ms/step - accuracy: 0.5224 - loss: 0.6920 - val_accuracy: 0.5393 - val_loss: 0.6907\nEpoch 4/20\n\u001B[1m3731/3731\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m85s\u001B[0m 23ms/step - accuracy: 0.5230 - loss: 0.6919 - val_accuracy: 0.5392 - val_loss: 0.6903\nEpoch 5/20\n\u001B[1m3731/3731\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m85s\u001B[0m 23ms/step - accuracy: 0.5226 - loss: 0.6919 - val_accuracy: 0.5389 - val_loss: 0.6903\nEpoch 5: early stopping\nRestoring model weights from the end of the best epoch: 2.\nTraining completed.\nStarting prediction...\nStarting feature calculation...\nRepartitioned data for parallel processing.\nCalculating returns and volume ratio...\nCalculating RSI...\nCalculating MACD...\nCalculating target column...\nFeature calculation complete. Number of rows: 909617\nPreparing LSTM data with sequence length 10...\nLSTM data preparation complete. Samples: 909607\n\u001B[1m28426/28426\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m79s\u001B[0m 3ms/step\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "0.49556",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
